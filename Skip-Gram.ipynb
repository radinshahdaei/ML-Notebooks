{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QOmoitc9qC4"
      },
      "source": [
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
        "<font color=0F5298 size=7>\n",
        "    Machine learning <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Fall 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 5 - NLP - Skip-Gram <br>\n",
        "<font color=0CBCDF size=4>\n",
        "   &#x1F335; Amirhossein Akbari  &#x1F335;\n",
        "</div>\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Y0nSE3-wuW"
      },
      "source": [
        "<font color=9999FF size=4>\n",
        "&#x1F388; Full Name : Radin Shahadei\n",
        "<br>\n",
        "<font color=9999FF size=4>\n",
        "&#x1F388; Student Number : 401106096"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4spZpsq_Pxf"
      },
      "source": [
        "<font color=0080FF size=3>\n",
        "This notebook explores word embeddings, compact and dense vector representations of words that capture their textual meaning. This notebook focusing on implementing the Word2Vec algorithm using the Skip-gram architecture and negative sampling.\n",
        "</font>\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "<br>\n",
        "<font color=66B2FF size=2>In this notebook, you are free to use any function or model from TensorFlow to assist with the implementation. However, PyTorch is not permitted for this exercise. This ensures consistency and alignment with the tools being focused on.</font>\n",
        "<br>\n",
        "<font color=red size=3>**Run All Cells Before Submission**</font>: <font color=FF99CC size=2>Before saving and submitting your notebook, please ensure you run all cells from start to finish. This practice guarantees that your notebook is self-consistent and can be evaluated correctly by others.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZCaUs-3FsJk"
      },
      "source": [
        "<font color=#ffb578 size=3>\n",
        "you are free to modify, add, or remove any cells as you see fit to complete your tasks. Feel free to change any of the provided code or content to better suit your understanding and approach to the problems.\n",
        "\n",
        "- **Questions**: If you have any questions or require clarifications as you work through the notebook, please do not hesitate to ask. You can post your queries on Quera or reach out via Telegram.\n",
        "- **Feedback**: We encourage you to seek feedback and engage in discussions to enhance your learning experience and improve your solutions.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:12:41.263643Z",
          "iopub.status.busy": "2020-10-10T13:12:41.262979Z",
          "iopub.status.idle": "2020-10-10T13:12:49.471771Z",
          "shell.execute_reply": "2020-10-10T13:12:49.471205Z"
        },
        "id": "hoV5vSSSbIp0",
        "outputId": "ddec5966-573a-415a-b0f4-652d31f59f49",
        "papermill": {
          "duration": 8.238801,
          "end_time": "2020-10-10T13:12:49.471884",
          "exception": false,
          "start_time": "2020-10-10T13:12:41.233083",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import math\n",
        "import gzip\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "import tensorflow_datasets as tfds\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pwuegqx-JWf",
        "papermill": {
          "duration": 0.023154,
          "end_time": "2020-10-10T13:12:49.521385",
          "exception": false,
          "start_time": "2020-10-10T13:12:49.498231",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Downloading Dataset\n",
        "We're going to use text8 dataset. Text8 is first 100,000,000 bytes of plain text from Wikipedia. It's mainly used for testing purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:12:49.580623Z",
          "iopub.status.busy": "2020-10-10T13:12:49.579765Z",
          "iopub.status.idle": "2020-10-10T13:13:03.778660Z",
          "shell.execute_reply": "2020-10-10T13:13:03.779347Z"
        },
        "id": "XG-FjuVEFLGW",
        "outputId": "3f947a19-d601-497e-e30b-031bda4f6cd0",
        "papermill": {
          "duration": 14.234694,
          "end_time": "2020-10-10T13:13:03.779495",
          "exception": false,
          "start_time": "2020-10-10T13:12:49.544801",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "def load_data():\n",
        "  text8_zip_file_path = api.load('text8', return_path=True)\n",
        "  with gzip.open(text8_zip_file_path, 'rb') as file:\n",
        "    file_content = file.read()\n",
        "  wiki = file_content.decode()\n",
        "  return wiki\n",
        "\n",
        "wiki = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBT_-Vy_af4",
        "papermill": {
          "duration": 1.449897,
          "end_time": "2020-10-10T13:13:06.611079",
          "exception": false,
          "start_time": "2020-10-10T13:13:05.161182",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "**Stopwords removal** - Begin by removing stopwords from the dataset, as they provide little to no value in learning word embeddings. Ensure your preprocessing pipeline filters out commonly used words such as \"the,\" \"and,\" or \"of\" that do not contribute to meaningful semantic relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Subsampling words** - In a large corpora, most frequent words can easily occur hundreds of millions of times and such words usually don't bring much information to the table.  It is of essential importance to cut down on their frequencies to mitigate the negative impact it adds. For example, co-occurrences of \"English\" and \"Spanish\" benefit much more than co-occurrences of \"English\" and \"the\" or \"Spanish\" and \"of\". To counter the imbalance between rare and frequent words Mikolov et. al came up with the following heuristic formula for determining probability to drop a particular word:\n",
        "\n",
        "![formula.png](data:image/png;base64,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)\n",
        "\n",
        "where t is threshold value (heuristically set to 1e-5) and f(w) is frequency of the word.\n",
        "\n",
        "Implement a subsampling mechanism to handle overly frequent words in the corpus. Use the heuristic formula provided by Mikolov et al. to calculate the probability of dropping a word based on its frequency. This step ensures the corpus maintains a balance between rare and frequent words, improving the quality of word co-occurrence relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Filtering words** - Filter out words that occur only once in the dataset, as they lack sufficient context to be represented effectively. Retain only those words that appear at least five times in the corpus to minimize noise and enhance the overall quality of the embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:09.551725Z",
          "iopub.status.busy": "2020-10-10T13:13:09.441732Z",
          "iopub.status.idle": "2020-10-10T13:13:11.452524Z",
          "shell.execute_reply": "2020-10-10T13:13:11.451989Z"
        },
        "id": "wp50T2OqA-7L",
        "papermill": {
          "duration": 3.4315,
          "end_time": "2020-10-10T13:13:11.452631",
          "exception": false,
          "start_time": "2020-10-10T13:13:08.021131",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Step 1: Replace punctuation with spaces to standardize the text\n",
        "    text = text.replace('.', ' ').replace(',', ' ').replace(';', ' ').replace(':', ' ').replace('!', ' ').replace('?', ' ')\n",
        "\n",
        "    # Step 2: Convert text to lowercase and remove unnecessary whitespaces\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Tokenize the text into individual words\n",
        "    words = text.split()\n",
        "\n",
        "    # Step 3: Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Step 4: Remove words with frequency less than 5\n",
        "    word_counts = Counter(filtered_words)\n",
        "    filtered_words = [word for word in filtered_words if word_counts[word] >= 5]\n",
        "\n",
        "    # Step 5: Subsample words using a threshold value (e.g., 1e-5)\n",
        "    t = 1e-5\n",
        "    total_words = sum(word_counts.values())\n",
        "    word_prob = {word: count / total_words for word, count in word_counts.items()}\n",
        "    subsampled_words = [\n",
        "        word for word in filtered_words\n",
        "        if random.random() > (1 - math.sqrt(t / word_prob[word])) if word in word_prob\n",
        "    ]\n",
        "\n",
        "    return subsampled_words, word_counts\n",
        "\n",
        "processed_words, word_counts = preprocess_text(wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4260B2ZFJ_EA",
        "papermill": {
          "duration": 1.777391,
          "end_time": "2020-10-10T13:13:14.603396",
          "exception": false,
          "start_time": "2020-10-10T13:13:12.826005",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "It's always a good idea to take a quick look at preprocessed sample before heading further - you might observe few things that if handled can enrich or correct your data. More like a validation step this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:17.665608Z",
          "iopub.status.busy": "2020-10-10T13:13:17.664211Z",
          "iopub.status.idle": "2020-10-10T13:13:17.667915Z",
          "shell.execute_reply": "2020-10-10T13:13:17.668396Z"
        },
        "id": "_oNvdt-v1dw0",
        "outputId": "27a8fb4f-810b-4794-dab9-0686f4ebf218",
        "papermill": {
          "duration": 1.689149,
          "end_time": "2020-10-10T13:13:17.668521",
          "exception": false,
          "start_time": "2020-10-10T13:13:15.979372",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample preprocessed words from index 1500 to 1550:\n",
            "['criticised', 'anarchists', 'others', 'tacitly', 'statist', 'bureaucratic', 'tendencies', 'dogmatic', 'facade', 'hypocrisy', 'sexist', 'racist', 'views', 'proudhon', 'bakunin', 'hypocrisy', 'inherent', 'dismiss', 'prejudices', 'anarchists', 'criticise', 'anarchism', 'anarchist', 'proudhon', 'cercle', 'proudhon', 'bryan', 'caplan', 'fascists', 'sympathizers', 'civil', 'illegitimate', 'coercion', 'faction', 'fascists', 'criticizes', 'willingness', 'cnt', 'statist', 'republican', 'payne', 'book', 'cnt', 'negotiations', 'fascist', 'noam', 'chomsky', 'anarchism', 'encountered', 'publicly']\n",
            "\n",
            "Total words after processing: 3845522\n"
          ]
        }
      ],
      "source": [
        "# Take a quick look at a slice of preprocessed words (e.g., index 1500 to 1550)\n",
        "def validate_preprocessed_data(processed_words, start_index, end_index):\n",
        "    print(f\"Sample preprocessed words from index {start_index} to {end_index}:\")\n",
        "    print(processed_words[start_index:end_index])\n",
        "\n",
        "# Assuming `processed_words` is the output from preprocess_text function\n",
        "validate_preprocessed_data(processed_words, start_index=1500, end_index=1550)\n",
        "\n",
        "print(\"\\nTotal words after processing:\", len(processed_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCkFtaa_KrTb",
        "papermill": {
          "duration": 1.426874,
          "end_time": "2020-10-10T13:13:20.673211",
          "exception": false,
          "start_time": "2020-10-10T13:13:19.246337",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Hyperparameters\n",
        "Setting a few hyperparamters required for gnerating batches and for deciding the size of word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:23.483005Z",
          "iopub.status.busy": "2020-10-10T13:13:23.482371Z",
          "iopub.status.idle": "2020-10-10T13:13:23.486821Z",
          "shell.execute_reply": "2020-10-10T13:13:23.486338Z"
        },
        "id": "mJLzBkSIKoMx",
        "papermill": {
          "duration": 1.447402,
          "end_time": "2020-10-10T13:13:23.486929",
          "exception": false,
          "start_time": "2020-10-10T13:13:22.039527",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 64\n",
        "BUFFER_SIZE = 100000\n",
        "BATCH_SIZE = 2048\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oO7N0ZsLofI",
        "papermill": {
          "duration": 1.481198,
          "end_time": "2020-10-10T13:13:26.663213",
          "exception": false,
          "start_time": "2020-10-10T13:13:25.182015",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Preparing TensorFlow Dataset using Skipgrams\n",
        "\n",
        "**Generating Skipgrams**\n",
        "\n",
        "Tokenize your preprocessed textual data and convert the words into their corresponding vectorized tokens. Then, use the `skipgrams` function provided by Keras to generate (word, context) pairs. Ensure the following steps are completed:\n",
        "\n",
        "- Generate positive samples: (word, word in the same window), with label 1.  \n",
        "- Generate negative samples: (word, random word from the vocabulary), with label 0.  \n",
        "\n",
        "Refer to Mikolov et al.'s paper, [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf), for more details on Skipgrams.\n",
        "\n",
        "---\n",
        "\n",
        "**Negative Sampling**\n",
        "\n",
        "For each input word, implement the negative sampling approach to optimize the training process. Transform the problem of predicting context words into independent binary classification tasks. For every (target, context) pair, generate random negative (target, ~context) samples. This step will reduce computational complexity and make training more efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:29.493673Z",
          "iopub.status.busy": "2020-10-10T13:13:29.483476Z",
          "iopub.status.idle": "2020-10-10T13:14:02.707005Z",
          "shell.execute_reply": "2020-10-10T13:14:02.705890Z"
        },
        "id": "Uq4-jfYOjonO",
        "outputId": "272f4d06-c23c-4490-9abe-3d07c3ab8fe2",
        "papermill": {
          "duration": 34.646886,
          "end_time": "2020-10-10T13:14:02.707124",
          "exception": false,
          "start_time": "2020-10-10T13:13:28.060238",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Preprocessing and tokenization...\n",
            "Step 2: Generating skipgram pairs and labels...\n",
            "Step 3: Splitting data into training and testing sets...\n",
            "Step 4: Creating TensorFlow datasets...\n",
            "Training dataset and validation dataset are ready.\n"
          ]
        }
      ],
      "source": [
        "# Function to preprocess and tokenize words\n",
        "def preprocess_and_tokenize(preprocessed_words):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(preprocessed_words)\n",
        "    word_index = tokenizer.word_index\n",
        "    index_word = {v: k for k, v in word_index.items()}\n",
        "    vocab_size = len(word_index) + 1\n",
        "    sequences = tokenizer.texts_to_sequences([preprocessed_words])[0]\n",
        "    return tokenizer, word_index, index_word, vocab_size, sequences\n",
        "\n",
        "# Function to generate skipgram pairs and labels\n",
        "def generate_skipgram_pairs(sequences, vocab_size, window_size=3, negative_samples=0.7):\n",
        "    skip_gram_pairs, labels = skipgrams(\n",
        "        sequences,\n",
        "        vocabulary_size=vocab_size,\n",
        "        window_size=window_size,\n",
        "        negative_samples=negative_samples\n",
        "    )\n",
        "    targets, contexts = zip(*skip_gram_pairs)\n",
        "    targets = np.array(targets, dtype=np.int32)\n",
        "    contexts = np.array(contexts, dtype=np.int32)\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "    return targets, contexts, labels\n",
        "\n",
        "# Function to split data into training and testing sets\n",
        "def split_data(targets, contexts, labels, train_split=0.9):\n",
        "    split_index = int(len(targets) * train_split)\n",
        "    train_targets, val_targets = targets[:split_index], targets[split_index:]\n",
        "    train_contexts, val_contexts = contexts[:split_index], contexts[split_index:]\n",
        "    train_labels, val_labels = labels[:split_index], labels[split_index:]\n",
        "    return train_targets, train_contexts, train_labels, val_targets, val_contexts, val_labels\n",
        "\n",
        "# Function to create TensorFlow datasets\n",
        "def create_tf_datasets(train_targets, train_contexts, train_labels, val_targets, val_contexts, val_labels, buffer_size, batch_size):\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(((train_targets, train_contexts), train_labels))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(((val_targets, val_contexts), val_labels))\n",
        "    val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Configuration\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 512\n",
        "WINDOW_SIZE = 3\n",
        "NEGATIVE_SAMPLES = 0.7\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "print(\"Step 1: Preprocessing and tokenization...\")\n",
        "tokenizer, word_index, index_word, vocab_size, sequences = preprocess_and_tokenize(processed_words)\n",
        "\n",
        "print(\"Step 2: Generating skipgram pairs and labels...\")\n",
        "targets, contexts, labels = generate_skipgram_pairs(sequences, vocab_size, WINDOW_SIZE, NEGATIVE_SAMPLES)\n",
        "\n",
        "print(\"Step 3: Splitting data into training and testing sets...\")\n",
        "train_targets, train_contexts, train_labels, val_targets, val_contexts, val_labels = split_data(targets, contexts, labels)\n",
        "\n",
        "print(\"Step 4: Creating TensorFlow datasets...\")\n",
        "train_dataset, val_dataset = create_tf_datasets(\n",
        "    train_targets, train_contexts, train_labels,\n",
        "    val_targets, val_contexts, val_labels,\n",
        "    BUFFER_SIZE, BATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"Training dataset and validation dataset are ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgCP6kr2cf1Y",
        "outputId": "2326bd0d-55f1-4315-c3b4-9b861fc44ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training batches: 68948\n",
            "Testing batches: 7660\n"
          ]
        }
      ],
      "source": [
        "print(\"Training batches:\", len(train_dataset))\n",
        "print(\"Testing batches:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUatOx50OXF1",
        "papermill": {
          "duration": 1.350656,
          "end_time": "2020-10-10T13:14:15.692486",
          "exception": false,
          "start_time": "2020-10-10T13:14:14.341830",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Building the Model\n",
        "\n",
        "Use the model subclassing method to build your model. While Sequential and Functional APIs are generally more suitable for most use cases, model subclassing allows you to create the model in an object-oriented way. Follow these steps:\n",
        "\n",
        "1. Define a custom model class by inheriting from `tf.keras.Model`.\n",
        "2. Implement the `__init__` method to define the layers of your model.\n",
        "3. Override the `call` method to define the forward pass of your model.\n",
        "4. Ensure that the model includes embedding layers, a skip-gram architecture, and any other necessary components for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:18.975942Z",
          "iopub.status.busy": "2020-10-10T13:14:18.969008Z",
          "iopub.status.idle": "2020-10-10T13:14:19.276913Z",
          "shell.execute_reply": "2020-10-10T13:14:19.276347Z"
        },
        "id": "6gLxFZ9Eu9Tw",
        "outputId": "c3ef112c-0cee-4e52-b406-8e6e9e440526",
        "papermill": {
          "duration": 1.935377,
          "end_time": "2020-10-10T13:14:19.277030",
          "exception": false,
          "start_time": "2020-10-10T13:14:17.341653",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Word2Vec model...\n",
            "Model summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'word2_vec', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"word2_vec\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"word2_vec\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ target_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ context_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ target_embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ context_embedding (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_dense (\u001b[38;5;33mDense\u001b[0m)                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing forward pass with sample inputs...\n",
            "Model output for sample inputs: [[[0.49726316]]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the Word2Vec model class\n",
        "class Word2Vec(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        \"\"\"\n",
        "        Initializes the Word2Vec model with embedding and dense layers.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "            embedding_dim (int): The dimensionality of the embedding vectors.\n",
        "        \"\"\"\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.target_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embedding_dim, name=\"target_embedding\"\n",
        "        )\n",
        "        self.context_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embedding_dim, name=\"context_embedding\"\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(1, activation=None, name=\"output_dense\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            inputs (tuple): A tuple containing target and context tensors.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The sigmoid-activated output for skip-gram predictions.\n",
        "        \"\"\"\n",
        "        target, context = inputs\n",
        "\n",
        "        # Embedding lookups for target and context words\n",
        "        target_vector = self.target_embedding(target)\n",
        "        context_vector = self.context_embedding(context)\n",
        "\n",
        "        # Compute the dot product between target and context embeddings\n",
        "        dot_product = tf.reduce_sum(target_vector * context_vector, axis=-1, keepdims=True)\n",
        "\n",
        "        # Pass the dot product through a dense layer\n",
        "        output = self.dense(dot_product)\n",
        "\n",
        "        # Apply sigmoid activation\n",
        "        return tf.nn.sigmoid(output)\n",
        "\n",
        "# Function to test the model structure and forward pass\n",
        "def test_word2vec_model(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Tests the Word2Vec model's structure and forward pass.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary.\n",
        "        embedding_dim (int): The dimensionality of the embedding vectors.\n",
        "    \"\"\"\n",
        "    print(\"Initializing Word2Vec model...\")\n",
        "    model = Word2Vec(vocab_size, embedding_dim)\n",
        "\n",
        "    print(\"Model summary:\")\n",
        "    try:\n",
        "        model.build(input_shape=[(None,), (None,)])  # Define input shape for the summary\n",
        "        model.summary()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating summary: {e}\")\n",
        "\n",
        "    print(\"Testing forward pass with sample inputs...\")\n",
        "    sample_inputs = (tf.constant([[1]]), tf.constant([[2]]))\n",
        "    output = model(sample_inputs)\n",
        "    print(f\"Model output for sample inputs: {output.numpy()}\")\n",
        "\n",
        "# Run the test\n",
        "test_word2vec_model(vocab_size, EMBEDDING_DIM)\n",
        "\n",
        "# Initialize the model\n",
        "model = Word2Vec(vocab_size=vocab_size, embedding_dim=EMBEDDING_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3SV3zv0pXG",
        "papermill": {
          "duration": 1.56129,
          "end_time": "2020-10-10T13:14:22.236946",
          "exception": false,
          "start_time": "2020-10-10T13:14:20.675656",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Loss function, Metrics and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:25.048870Z",
          "iopub.status.busy": "2020-10-10T13:14:25.048241Z",
          "iopub.status.idle": "2020-10-10T13:14:25.068088Z",
          "shell.execute_reply": "2020-10-10T13:14:25.067312Z"
        },
        "id": "ENLrMWOtpixA",
        "papermill": {
          "duration": 1.420264,
          "end_time": "2020-10-10T13:14:25.068193",
          "exception": false,
          "start_time": "2020-10-10T13:14:23.647929",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "optimiser = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eyQ_o1EWuJA",
        "papermill": {
          "duration": 1.384257,
          "end_time": "2020-10-10T13:14:27.862984",
          "exception": false,
          "start_time": "2020-10-10T13:14:26.478727",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "Implement custom training for learning word embeddings to gain finer control over optimization and training tasks. Follow these steps:\n",
        "\n",
        "1. Define a custom training loop that includes forward propagation, loss computation, and backpropagation.\n",
        "2. Use the optimizer of your choice to update the model's weights based on the computed gradients.\n",
        "3. Implement batching for efficient data processing during training.\n",
        "4. Monitor the loss during each epoch to track the model's performance.\n",
        "5. Save the trained embeddings for later use once the training is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:30.800520Z",
          "iopub.status.busy": "2020-10-10T13:14:30.799563Z",
          "iopub.status.idle": "2020-10-10T13:36:46.204754Z",
          "shell.execute_reply": "2020-10-10T13:36:46.205408Z"
        },
        "id": "oHNb85OL29hu",
        "outputId": "6471b55a-4544-4522-e372-d1f6a74eda8e",
        "papermill": {
          "duration": 1336.991023,
          "end_time": "2020-10-10T13:36:46.205587",
          "exception": false,
          "start_time": "2020-10-10T13:14:29.214564",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 68948/68948 [13:13<00:00, 86.87it/s, Loss=0.427]\n",
            "Validation: 100%|██████████| 7660/7660 [01:21<00:00, 93.50it/s, Val Loss=0.45] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Train Acc: 0.7825, Val Acc: 0.8051, Time: 875.63s\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 68948/68948 [13:17<00:00, 86.46it/s, Loss=0.391]\n",
            "Validation: 100%|██████████| 7660/7660 [01:21<00:00, 93.53it/s, Val Loss=0.427] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Train Acc: 0.8162, Val Acc: 0.8151, Time: 879.34s\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 68948/68948 [13:52<00:00, 82.81it/s, Loss=0.369]\n",
            "Validation: 100%|██████████| 7660/7660 [01:21<00:00, 93.52it/s, Val Loss=0.418] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Train Acc: 0.8271, Val Acc: 0.8195, Time: 914.48s\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 68948/68948 [14:21<00:00, 79.99it/s, Loss=0.337]\n",
            "Validation: 100%|██████████| 7660/7660 [00:49<00:00, 155.10it/s, Val Loss=0.413]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Train Acc: 0.8351, Val Acc: 0.8226, Time: 911.32s\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  67%|██████▋   | 46301/68948 [09:03<04:14, 89.11it/s, Loss=0.413]"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Define the training step\n",
        "@tf.function\n",
        "def train_step(x, y, model, loss_fn, optimizer, train_acc_metric):\n",
        "    \"\"\"\n",
        "    Executes one training step: forward pass, loss computation, and weight updates.\n",
        "\n",
        "    Args:\n",
        "        x: Input data (target and context pairs).\n",
        "        y: Ground truth labels (positive/negative).\n",
        "        model: The Word2Vec model.\n",
        "        loss_fn: Loss function for training.\n",
        "        optimizer: Optimizer for weight updates.\n",
        "        train_acc_metric: Metric for tracking training accuracy.\n",
        "\n",
        "    Returns:\n",
        "        loss: Computed training loss.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x, training=True)\n",
        "        loss = loss_fn(y, y_pred)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y, y_pred)\n",
        "    return loss\n",
        "\n",
        "# Step 2: Define the testing step\n",
        "@tf.function\n",
        "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
        "    \"\"\"\n",
        "    Executes one testing step: forward pass and loss computation without weight updates.\n",
        "\n",
        "    Args:\n",
        "        x: Input data (target and context pairs).\n",
        "        y: Ground truth labels (positive/negative).\n",
        "        model: The Word2Vec model.\n",
        "        loss_fn: Loss function for validation.\n",
        "        val_acc_metric: Metric for tracking validation accuracy.\n",
        "\n",
        "    Returns:\n",
        "        loss: Computed validation loss.\n",
        "    \"\"\"\n",
        "    y_pred = model(x, training=False)\n",
        "    loss = loss_fn(y, y_pred)\n",
        "    val_acc_metric.update_state(y, y_pred)\n",
        "    return loss\n",
        "\n",
        "# Step 3: Initialize the training loop\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 5: Compute and display training accuracy\n",
        "    train_acc_metric.reset_state()\n",
        "    val_acc_metric.reset_state()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    # Step 4: Perform training on each batch\n",
        "    train_progress = tqdm(train_dataset, desc=\"Training\")\n",
        "    for (x_train, y_train) in train_progress:\n",
        "        loss = train_step(x_train, y_train, model, loss_fn, optimiser, train_acc_metric)\n",
        "        train_progress.set_postfix({'Loss': loss.numpy()})\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "\n",
        "    # Step 6: Perform validation on the test dataset\n",
        "    val_progress = tqdm(val_dataset, desc=\"Validation\")\n",
        "    for (x_val, y_val) in val_progress:\n",
        "        val_loss = test_step(x_val, y_val, model, loss_fn, val_acc_metric)\n",
        "        val_progress.set_postfix({'Val Loss': val_loss.numpy()})\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "\n",
        "    # Step 7: Log validation metrics\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\n",
        "        f\" - Train Acc: {train_acc.numpy():.4f}, \"\n",
        "        f\"Val Acc: {val_acc.numpy():.4f}, \"\n",
        "        f\"Time: {elapsed_time:.2f}s\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output cell wasn't saved propperly, the last epoch is like this:\n",
        "```\n",
        "Epoch 5/5\n",
        "  Training: 100%|██████████| 68948/68948 [13:11<00:00, 87.11it/s, Loss=0.322]\n",
        "  Validation: 100%|██████████| 7660/7660 [00:40<00:00, 187.08it/s, Val Loss=0.412]\n",
        "  - Train Acc: 0.8422, Val Acc: 0.8249, Time: 832.46s\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:36:48.935958Z",
          "iopub.status.busy": "2020-10-10T13:36:48.935357Z",
          "iopub.status.idle": "2020-10-10T13:36:48.973240Z",
          "shell.execute_reply": "2020-10-10T13:36:48.972739Z"
        },
        "id": "V2iWMNGQahsc",
        "outputId": "dddf9e0c-b030-4ab4-a94d-f1ea6fa55176",
        "papermill": {
          "duration": 1.409531,
          "end_time": "2020-10-10T13:36:48.973383",
          "exception": false,
          "start_time": "2020-10-10T13:36:47.563852",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'model/ckpt-1'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint.save('model/ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7Wx_rzZOOm",
        "papermill": {
          "duration": 1.591393,
          "end_time": "2020-10-10T13:36:51.938622",
          "exception": false,
          "start_time": "2020-10-10T13:36:50.347229",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Word Embeddings Projector\n",
        "\n",
        "Follow these steps to visualize the learned word embeddings using TensorFlow's Embedding Projector:\n",
        "\n",
        "1. Extract the weights of the embedding layer from your trained model.\n",
        "2. Save the weights into two files:\n",
        "   - `vecs.tsv`: This file will store the actual vector representations of words.\n",
        "   - `meta.tsv`: This file will store the associated metadata (e.g., word labels) for visualization.\n",
        "3. Go to [TensorFlow Embedding Projector](http://projector.tensorflow.org/).\n",
        "4. Upload the `vecs.tsv` and `meta.tsv` files created in the previous step.\n",
        "5. Explore the visualizations provided by TensorFlow's Embedding Projector.\n",
        "<font color=#ffb578>\n",
        "6.Save the visualization of a word that best demonstrate the quality of your embeddings as an image and store it near the notebook.\n",
        "7. Compress the folder into a `.zip` file and submit it as part of your work.\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-10T13:36:54.675412Z",
          "iopub.status.busy": "2020-10-10T13:36:54.674309Z",
          "iopub.status.idle": "2020-10-10T13:36:56.018539Z",
          "shell.execute_reply": "2020-10-10T13:36:56.017942Z"
        },
        "id": "fGpXtNRS-V_u",
        "outputId": "91a0d1cc-77bf-4f06-db20-add2207dbc6e",
        "papermill": {
          "duration": 2.703171,
          "end_time": "2020-10-10T13:36:56.018656",
          "exception": false,
          "start_time": "2020-10-10T13:36:53.315485",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: (71141, 64)\n"
          ]
        }
      ],
      "source": [
        "embeddings = model.target_embedding.get_weights()[0]\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "with open('vecs.tsv', 'w') as vec_file, open('meta.tsv', 'w') as meta_file:\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        if idx < vocab_size:\n",
        "            vector = embeddings[idx]\n",
        "            vec_file.write('\\t'.join([str(x) for x in vector]) + '\\n')\n",
        "            meta_file.write(word + '\\n')\n",
        "\n",
        "vec_file.close()\n",
        "meta_file.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 1467.163823,
      "end_time": "2020-10-10T13:37:04.319726",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-10-10T13:12:37.155903",
      "version": "2.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
